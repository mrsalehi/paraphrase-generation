include "optim/default.txt"
include "eval/short.txt"

seed = 0

model_dir = im_v4_quora_naug4

editor {
  use_pretrained_embeddings = False
  use_sub_words = True
  use_t2t_sub_words = True
  t2t_sub_words_vocab_path = sub_word_vocab.txt

  initializer_gain = 1.0

  vocab_size = 5000
  word_dim = 128

  dropout_keep = 0.7
  dropout = 0.3
  enable_dropout = True

  norm_max = 14.0
  kill_edit = False
  draw_edit = False

  use_free_set = True
  wvec_path = glove.6B.300d_dbpedia.txt

  transformer {
    hidden_size = 128  # Model dimension in the hidden layers.
    num_hidden_layers = 6  # Number of layer stacks.
    num_heads = 8  # Number of heads to use in multi-headed attention.
    filter_size = 512  # Inner layer dimension in the feedforward network.

    # Dropout values (only used when training)
    enable_dropout = True
    layer_postprocess_dropout = 0.3
    attention_dropout = 0.3
    relu_dropout = 0.3
    allow_ffn_pad = True
  }

  encoder {
    # Uncomment if you need to have diffrent values from transformer
    # hidden_size = 200
    # num_hidden_layers = 4
    # num_heads = 6
    # filter_size = 256
  }

  decoder {
    # Uncomment if you need to have diffrent values from transformer
    # hidden_size = 200
    # num_hidden_layers = 4
    # num_heads = 6
    # filter_size = 400

    allow_mev_st_attn = True
    allow_mev_ts_attn = False

    max_decode_length = 60

    beam_size = 5
    beam_decoding_alpha = 0.85
  }

  edit_encoder {
    edit_dim = 128
    micro_ev_dim = 64

    mev_proj_activation_fn = tanh
    # edit_vector_proj_activation_fn = tanh

    word_acc {
      accumulated_dim = 64
    }

    extractor {
      hidden_size = 128
      filter_size = 512

      noiser_ident_prob = 1

      encoder {
        num_hidden_layers = 4
        num_heads = 8
      }

      decoder {
        num_hidden_layers = 4
        num_heads = 8
      }
    }

  }
}

dataset {
    # this path should be relative to $SQUAD_ENTAILMENT_DATA
    path = quora_naug
    use_diff = True
}

