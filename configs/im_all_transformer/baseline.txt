include "optim/default.txt"
include "eval/short.txt"

seed = 0

model_dir = im_v4_quora_naug4

editor {
    transformer {
        # Model params
        hidden_size = 50  # Model dimension in the hidden layers.
        num_hidden_layers = 4  # Number of layers in the encoder and decoder stacks.
        num_heads = 5  # Number of heads to use in multi-headed attention.
        filter_size = 256  # Inner layer dimension in the feedforward network.

        # Dropout values (only used when training)
        enable_dropout = True
        layer_postprocess_dropout = 0.1
        attention_dropout = 0.1
        relu_dropout = 0.1
        allow_ffn_pad = True
        initializer_gain = 1.0
    }

    base_sent_encoder {

    }

    use_pretrained_embeddings = True
    use_sub_words = True

    vocab_size = 5000  # a proper size would be >20000
    word_dim = 50
    hidden_dim = 256  # hidden state dim of encoder and decoder
    agenda_dim = 256  # agenda vector dim
    edit_dim = 128 # edit vector dimension
    attention_dim = 128
    max_sent_length = 40
    encoder_layers = 3
    decoder_layers = 3
    beam_width = 5

    dropout_keep = 0.8
    enable_dropout = True

    layer_postprocess_dropout = 0.1
    attention_dropout = 0.1
    relu_dropout = 0.1

    ident_pr = 0.1
    attend_pr = 0.0
    enable_vae = True
    lamb_reg = 100.0
    norm_eps = 0.1
    norm_max = 14.0
    kill_edit = False
    draw_edit = False
    use_swap_memory = True
    use_free_set = False
    embed_sentence = False
    use_beam_decoder = False
    wvec_path = glove.6B.300d_dbpedia.txt

    reconstruction_dense_layers = [192]
    recons_temp {
        starter = 0.9
        decay_rate = 0.6
        decay_steps = 10000
    }

    reconstruction {
        agenda = 64

        encoder {
            hidden = 128
            layers = 2
        }

        decoder {
            hidden = 128
            layers = 2
        }
    }

    edit_encoder {
        edit_dim = 128
        wa_dim = 128
        micro_ev_dim = 128
        mev_proj_activation_fn = tanh

        transformer {
            hidden_size = 128
            filter_size = 256

            encoder {
                num_hidden_layers = 2
                num_heads = 4
            }

            decoder {
                num_hidden_layers = 2
                filter_size = 512
                num_heads = 2
            }
        }
    }
}

dataset {
    # this path should be relative to $SQUAD_ENTAILMENT_DATA
    path = quora_naug
    use_diff = True
}

