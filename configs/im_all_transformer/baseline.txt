include "optim/default.txt"
include "eval/short.txt"

seed = 0

model_dir = im_v4_quora_naug4

editor {
    transformer {
        # Model params
        hidden_size = 50  # Model dimension in the hidden layers.
        num_hidden_layers = 4  # Number of layers in the encoder and decoder stacks.
        num_heads = 5  # Number of heads to use in multi-headed attention.
        filter_size = 256  # Inner layer dimension in the feedforward network.

        # Dropout values (only used when training)
        enable_dropout = True
        layer_postprocess_dropout = 0.1
        attention_dropout = 0.1
        relu_dropout = 0.1
        allow_ffn_pad = True
    }

    base_sent_encoder {

    }

    use_pretrained_embeddings = True
    use_sub_words = True

    vocab_size = 5000  # a proper size would be >20000
    word_dim = 50
    hidden_dim = 256  # hidden state dim of encoder and decoder
    agenda_dim = 256  # agenda vector dim
    edit_dim = 128 # edit vector dimension
    attention_dim = 128
    max_sent_length = 40
    encoder_layers = 3
    decoder_layers = 3
    beam_width = 5

    dropout_keep = 0.8
    enable_dropout = True
    layer_postprocess_dropout = 0.1
    attention_dropout = 0.1
    relu_dropout = 0.1

    ident_pr = 0.1
    attend_pr = 0.0
    enable_vae = True
    lamb_reg = 100.0
    norm_eps = 0.1
    norm_max = 14.0
    kill_edit = False
    draw_edit = False
    use_swap_memory = True
    use_free_set = False
    embed_sentence = False
    use_beam_decoder = False
    wvec_path = glove.6B.300d_dbpedia.txt

    reconstruction_dense_layers = [192]
    recons_temp {
        starter = 0.9
        decay_rate = 0.6
        decay_steps = 10000
    }

    reconstruction {
        agenda = 64

        encoder {
            hidden = 128
            layers = 2
        }

        decoder {
            hidden = 128
            layers = 2
        }
    }

    edit_enc {
        ctx_hidden_dim = 256
        ctx_hidden_layer = 3
        wa_hidden_dim = 128
        wa_hidden_layer = 1
        meve_hidden_dim = 128
        meve_hidden_layer = 1
        num_heads = 4
        micro_ev_dim = 128

        transformer {
            # Model params
            initializer_gain = 1.0  # Used in trainable variable initialization.
            hidden_size = 128  # Model dimension in the hidden layers.
            num_hidden_layers = 4  # Number of layers in the encoder and decoder stacks.
            num_heads = 4  # Number of heads to use in multi-headed attention.
            filter_size = 256  # Inner layer dimension in the feedforward network.
            pos_encoding_dim = 64

            # Dropout values (only used when training)
            enable_dropout = True
            layer_postprocess_dropout = 0.1
            attention_dropout = 0.1
            relu_dropout = 0.1

            # Training params
            label_smoothing = 0.1
            learning_rate = 2.0
            learning_rate_decay_rate = 1.0
            learning_rate_warmup_steps = 16000

            # Optimizer params
            optimizer_adam_beta1 = 0.9
            optimizer_adam_beta2 = 0.997
            optimizer_adam_epsilon = 1e-09

            allow_ffn_pad=True
        }
    }
}

dataset {
    # this path should be relative to $SQUAD_ENTAILMENT_DATA
    path = quora_naug
    use_diff = True
}

